"""
Title: Calculate ancient IBD between a modern individual and ancient ones
Date:2025-04-09
Author: Tongrui Zhang

Description:
    This workflow will calculate ancient IBD between a modern individual and ancient ones. 
    The main input is a 23andMe format file for modern individual and several bam files for ancient individuals. Reference files are also required.
    The output is a tsv file reporting IBD segments found between modern individual and ancient ones by genomic region (chr, start, end).
    Results sorted by the highest total share IBD.
    The workflow is based on the ancIBD pipeline, and uses the ATLAS, PLINK and BCFtools, and GLIMPSE2 tools to process the original data so that it can be used by the ancIBD pipeline.
    
List of rules:
    1.ref_genome_hg19:  
        Process the reference genome file, remove "chr" from headers of the reference genome fasta file to get appropriate chromosome names that match other files chromosome names.
    2.atlas_genotype_likelihood:
        Calculate the genotype likelihoods which is required by GLIMPSE2 and ancIBD for each ancient bam files by the tool atlas with the reference genome.
    3.convert_23andme_to_vcf:
        Convert the 23andMe file of modern individual to vcf format with PL (Phred-scaled genotype likelihoods) information which is required by GLIMPSE2 and ancIBD.
    4.split_anc_vcf:
        Split the ancient vcf files with genotype likelihoods information (generated by atlas) by chromosome, only keep chromosome 1-22.
    5.split_modern_vcf_by_chr:
        Split the modern vcf files with PL information by chromosome, only keep chromosome 1-22.
    6.glimpse_reference_panel:
        Prepare the reference panel for GLIMPSE2 to increase efficiency and do basic QC step.
    7.glimpse_chunk:
        Define the chunks where to run imputation and phasing with GLIMPSE2_chunk.
    8.glimpse_split_ref:
        Create binary reference panel for each imputed chunk.
    9.glimpse_phase_ancient:
        Impute and phase for each chromosome for ancient individuals with GLIMPSE2_phase.
    10.glimpse_phase_modern:
        Impute and phase for each chromosome for modern individuals with GLIMPSE2_phase.
    11.glimpse_ligate_ancient:
        Ligation of the imputed chunks for ancient files is performed using the GLIMPSE2_ligate with an ordered list of the imputed chunks.
    12.glimpse_ligate_modern:
        Ligation of the imputed chunks for modern file is performed using the GLIMPSE2_ligate with an ordered list of the imputed chunks.
    13.merge_vcf_chr:
        Create a merged vcf file for each chromosome containing information for all ancient and modern individuals as it's required by ancIBD.
    14.vcf_to_hdf5:
        Convert the merged vcf file to hdf5 format which is required by ancIBD.
    15.call_ibd:
        Run ancIBD on samples to call IBD and get output for all IBD segments.
    16.combine_ibd_results:
        Combine IBD calls from all chromosomes and postprocess output into single summary table.
    17.process_ibd_results:
        Sort and merge the results to get the final output.
        
Procedure:
    1. Use ATLAS to call genotype likelihoods for ancient bam files
    2. Convert the 23andMe file of modern individual to vcf format with PL information with PLINK and BCFtools
    3. Use GLIMPSE2 to process ancient and modern files for imputation and phasing
    4. Use ancIBD to call IBD segments
    5. Process the IBD results and generate a final report

Usage:
    snakemake --cores 128
"""
###########################################################################################################################################################################################

import os
import pandas as pd

configfile: 'config/config.yaml'

REF_NOCHR = config['ref_genome_ori'].replace(".fa", "_nochr.fa")
anc_sample_df = (pd.read_csv(config['anc_samples'],
        sep='\t',
        dtype={'sample_name':str, 'bam':str})
    .set_index('sample_name', drop=False))
mdsample = os.path.basename(config['modern_sample']).replace('.txt', '')

rule all:
    input:
        REF_NOCHR,  
        expand("results/03_ancient/{ancsample}/split_chromosomes/ligate/{ancsample}_chr{chr}_ligated.vcf.gz", 
               ancsample=anc_sample_df['sample_name'], chr=range(1, 23)),
        expand("results/02_modern/{mdsample}/split_chromosomes/ligate/{mdsample}_chr{chr}_ligated.vcf.gz", 
               mdsample=mdsample, chr=range(1, 23)),
        expand("results/05_h5/chr{chr}.vcf.gz", chr=range(1, 23)),
        expand("results/05_h5/all_chr{chr}.h5", chr=range(1, 23)),
        expand("results/05_h5/1240k_chr{chr}.vcf.gz", chr=range(1, 23)),
        expand("results/06_ibd/ch{chr}.tsv", chr=range(1, 23)),
        "results/06_ibd/ch_all.tsv",
        "results/06_ibd/ibd_ind.d220.tsv",
        f"results/06_ibd/processed_ibd_report_{mdsample}.tsv"

#remove "chr" from the reference genome fasta file to get appropriate chromosome names
#this rule helps to make chromosome naming consistent across files
rule ref_genome_hg19:
    input:
        ref_genome = config['ref_genome_ori']
    output:
        ref_nochr_genome = REF_NOCHR
    shell:
        """
        seqkit replace -p "^chr" -r "" {input.ref_genome} > {output.ref_nochr_genome}
        """

#calculate the genotype likelihoods for each ancient sample by atlas with the reference genome and generate vcf files for ancient samples
#genotype likelihoods are required by GLIMPSE2 and ancIBD
rule atlas_genotype_likelihood:
    input:
        bam = lambda wildcards: anc_sample_df.loc[wildcards.ancsample, 'bam'],
        ref = rules.ref_genome_hg19.output.ref_nochr_genome
    output:
        vcf = "results/01_atlas/{ancsample}_calls_maximumLikelihood.vcf.gz"
    log:
        out = "logs/atlas/{ancsample}.out",
        err = "logs/atlas/{ancsample}.err"
    params:
        out_prefix = "results/01_atlas/{ancsample}"
    shell:
        """
        mkdir -p results/01_atlas logs/atlas
        
        atlas --task call \
            --bam {input.bam} \
            --fasta {input.ref} \
            --fixedSeed 1 \
            --out {params.out_prefix} \
            --logFile {log.out} 2> {log.err}
        """

#convert the 23andMe file of modern individual to vcf format with PL information
#PL is required by GLIMPSE2 and ancIBD
rule convert_23andme_to_vcf:
    input:
        modern_file = "resources/testind/{mdsample}.txt",
        pl_header = "resources/testind/pl_header.txt"
    output:
        vcf = "results/02_modern/{mdsample}.vcf",
        pl_vcf = "results/02_modern/{mdsample}_pl.vcf"
    params:
        plink_prefix = "results/02_modern/{mdsample}_plink"
    shell:
        """
        mkdir -p results/02_modern logs/modern
        # 23andMe to plink
        plink --23file {input.modern_file} --out {params.plink_prefix}
        
        # plink to vcf
        plink --bfile {params.plink_prefix} --recode vcf --out results/02_modern/{wildcards.mdsample}
        
        # add PL related information to the vcf file as PL is required by GLIMPSE2
        # add 0,99,99 for 0/0, 99,0,99 for 0/1 and 99,99,0 for 1/1
        # 23andMe file has very high quality for genotype information, so we can set 0 for corresponding genotype to show the high possibility for this genotype with confidence and 99 for the other two genotypes
        bcftools annotate -h {input.pl_header} {output.vcf} | \
        awk 'BEGIN{{FS=OFS="\\t"}} 
             /^#/ {{print}} 
             !/^#/ {{$9="GT:PL"; 
                    if ($10=="0/0") $10="0/0:0,99,99"; 
                    else if ($10=="0/1") $10="0/1:99,0,99"; 
                    else if ($10=="1/1") $10="1/1:99,99,0"; 
                    print}}' > {output.pl_vcf}
        """

#split the ancient vcf file with genotype likelihoods by chromosome, only keep chromosome 1-22 as ancIBD only needs these chromosomes
rule split_anc_vcf:
    input:
        vcf_bs = rules.atlas_genotype_likelihood.output.vcf
    output:
        expand("results/03_ancient/{{ancsample}}/split_chromosomes/{{ancsample}}.chr{chrom}.vcf.gz", chrom=range(1, 23)),
        expand("results/03_ancient/{{ancsample}}/split_chromosomes/{{ancsample}}.chr{chrom}.vcf.gz.tbi", chrom=range(1, 23))
    shell:
        """
        mkdir -p results/03_ancient/{wildcards.ancsample}/split_chromosomes
        mkdir -p temp1
        # use bgzip to regenerate the vcf.gz file and get related index
        gunzip -c {input.vcf_bs} > temp1/temp_{wildcards.ancsample}.vcf
        bgzip temp1/temp_{wildcards.ancsample}.vcf
        tabix -p vcf temp1/temp_{wildcards.ancsample}.vcf.gz

        # use for loop to split every vcf.gz file of asncient individual into 22 separate files by 22 chromosomes
        for chr in {{1..22}}; do
            bcftools view temp1/temp_{wildcards.ancsample}.vcf.gz -r $chr -O z -o results/03_ancient/{wildcards.ancsample}/split_chromosomes/{wildcards.ancsample}.chr${{chr}}.vcf.gz
            tabix -p vcf results/03_ancient/{wildcards.ancsample}/split_chromosomes/{wildcards.ancsample}.chr${{chr}}.vcf.gz
        done
            
        # clean temp files
        rm temp1/temp_{wildcards.ancsample}.vcf.gz*
        """

#split the modern vcf file with PL information by chromosome, only keep chromosome 1-22 as ancIBD only needs these chromosomes
rule split_modern_vcf_by_chr:
    input:
        vcf_bs = rules.convert_23andme_to_vcf.output.pl_vcf
    output:
        expand("results/02_modern/{{mdsample}}/split_chromosomes/{{mdsample}}.chr{chrom}.vcf.gz", chrom=range(1, 23)),
        expand("results/02_modern/{{mdsample}}/split_chromosomes/{{mdsample}}.chr{chrom}.vcf.gz.tbi", chrom=range(1, 23))
    shell:
        """
        mkdir -p results/02_modern/{wildcards.mdsample}/split_chromosomes
        mkdir -p temp2

        bgzip -c {input.vcf_bs} > temp2/temp_{wildcards.mdsample}.vcf.gz
        tabix -p vcf temp2/temp_{wildcards.mdsample}.vcf.gz
        
        for chr in {{1..22}}; do
            bcftools view temp2/temp_{wildcards.mdsample}.vcf.gz -r $chr -O z -o results/02_modern/{wildcards.mdsample}/split_chromosomes/{wildcards.mdsample}.chr${{chr}}.vcf.gz
            tabix -p vcf results/02_modern/{wildcards.mdsample}/split_chromosomes/{wildcards.mdsample}.chr${{chr}}.vcf.gz
        done
        
        rm temp2/temp_{wildcards.mdsample}.vcf.gz*
        """

#prepare the reference panel for GLIMPSE2 according to tutorial of GLIMPSE2. 
#export the dataset in BCF file format for efficiency reasons.
#performs a basic QC step by keeping only SNPs and remove multiallelic records.
rule glimpse_reference_panel:
    input:
        panel = "resources/ref_panel/ALL.chr{chr}.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz"
    output:
        bcf = "results/04_refpanel/chr{chr}.bcf",
        bcf_index = "results/04_refpanel/chr{chr}.bcf.csi",
        f_bcf = "results/04_refpanel/chr{chr}_f.bcf",
        f_bcf_index = "results/04_refpanel/chr{chr}_f.bcf.csi",
        f_sites_vcf = "results/04_refpanel/chr{chr}_f.sites.vcf.gz",
        f_sites_vcf_index = "results/04_refpanel/chr{chr}_f.sites.vcf.gz.csi"
    threads: 32
    shell:
        """
        mkdir -p results/04_refpanel
        
        bcftools index -f {input.panel}

        bcftools norm -m -any {input.panel} -Ob --threads {threads} -o {output.bcf}
        bcftools index -f {output.bcf} --threads {threads}

        bcftools view -m 2 -M 2 -v snps --threads {threads} -Ob -o {output.f_bcf} {output.bcf}
        bcftools index -f --threads {threads} {output.f_bcf}

        bcftools view -G -Oz -o {output.f_sites_vcf} {output.f_bcf}
        bcftools index -f --threads {threads} {output.f_sites_vcf}

        """
#split the genome into chunks
#define the chunks where to run imputation and phasing with GLIMPSE2_chunk
rule glimpse_chunk:
    input:
        ref_panel = rules.glimpse_reference_panel.output.f_sites_vcf
    output:
        chunks = "results/04_refpanel/chunks.chr{chr}.txt"
    shell:
        """        
        GLIMPSE2_chunk --input {input.ref_panel} \
                      --region {wildcards.chr} \
                      --output {output.chunks} \
                      --map resources/gmap/chr{wildcards.chr}.b37.gmap.gz \
                      --sequential
        """

#create binary reference panel
#convert the reference panel into GLIMPSE2’s binary file format. 
#input data of this step are the reference panel of haplotypes, the genetic map and the imputation regions computed in the previous step
rule glimpse_split_ref:
   input:
       ref = rules.glimpse_reference_panel.output.f_bcf,
       gmap = "resources/gmap/chr{chr}.b37.gmap.gz",
       chunks = rules.glimpse_chunk.output.chunks
   output:
       bi_rp_dir = directory("results/04_refpanel/split_ref_chr{chr}")
   shell:
       """
       mkdir -p {output.bi_rp_dir}
      
       while IFS="" read -r LINE || [ -n "$LINE" ]; do
           printf -v ID "%02d" $(echo $LINE | cut -d" " -f1)
           IRG=$(echo $LINE | cut -d" " -f3)
           ORG=$(echo $LINE | cut -d" " -f4)
          
           GLIMPSE2_split_reference --reference {input.ref} \
               --map {input.gmap} \
               --input-region $IRG \
               --output-region $ORG \
               --output {output.bi_rp_dir}/1000GP.chr{wildcards.chr}
       done < {input.chunks}
       """

#impute and phase for each chromosome for ancient individuas
#the algorithm works by iteratively refining the genotype likelihoods of the target individuals
#the output of the method is a VCF file containing: the best guess genotype in the FORMAT/GT field, the imputed genotype dosage in the FORMAT/DS field and the genotype probabilities in the FORMAT/GP field
rule glimpse_phase_ancient:
    input:
        ref = rules.glimpse_split_ref.output.bi_rp_dir,
        chunks = rules.glimpse_chunk.output.chunks,
        vcf = "results/03_ancient/{ancsample}/split_chromosomes/{ancsample}.chr{chr}.vcf.gz"
    output:
        anc_p = directory("results/03_ancient/{ancsample}/split_chromosomes/chr{chr}")
    shell:
        """    
        mkdir -p {output.anc_p}
        while IFS="" read -r LINE || [ -n "$LINE" ]; do
            printf -v ID "%02d" $(echo $LINE | cut -d" " -f1)
            IRG=$(echo $LINE | cut -d" " -f3)
            ORG=$(echo $LINE | cut -d" " -f4)
            CHR=$(echo $LINE | cut -d" " -f2)
            REGS=$(echo $IRG | cut -d":" -f 2 | cut -d"-" -f1)
            REGE=$(echo $IRG | cut -d":" -f 2 | cut -d"-" -f2)
            
            REF={input.ref}/1000GP.chr{wildcards.chr}
            
            # run GLIMPSE2_phase
            GLIMPSE2_phase --input-gl {input.vcf} \
                --reference ${{REF}}_${{CHR}}_${{REGS}}_${{REGE}}.bin \
                --output {output.anc_p}/chr{wildcards.chr}_imputed_${{CHR}}_${{REGS}}_${{REGE}}.bcf
        done < {input.chunks}
        
        """

#impute and phase for each chromosome for modern individuas
#we process the modern individual and ancient ones separately, since the path for these 2 types of files are slightly different (in 02_modern/03_ancient folder)and it's easier to manage them separately
rule glimpse_phase_modern:
    input:
        ref = rules.glimpse_split_ref.output.bi_rp_dir,
        chunks = rules.glimpse_chunk.output.chunks,
        vcf = "results/02_modern/{mdsample}/split_chromosomes/{mdsample}.chr{chr}.vcf.gz"
    output:
        mod_p = directory("results/02_modern/{mdsample}/split_chromosomes/chr{chr}")
    shell:
        """    
        mkdir -p {output.mod_p}
        while IFS="" read -r LINE || [ -n "$LINE" ]; do
            printf -v ID "%02d" $(echo $LINE | cut -d" " -f1)
            IRG=$(echo $LINE | cut -d" " -f3)
            ORG=$(echo $LINE | cut -d" " -f4)
            CHR=$(echo $LINE | cut -d" " -f2)
            REGS=$(echo $IRG | cut -d":" -f 2 | cut -d"-" -f1)
            REGE=$(echo $IRG | cut -d":" -f 2 | cut -d"-" -f2)
            
            REF={input.ref}/1000GP.chr{wildcards.chr}
            
            # run GLIMPSE2_phase
            GLIMPSE2_phase --input-gl {input.vcf} \
                --reference ${{REF}}_${{CHR}}_${{REGS}}_${{REGE}}.bin \
                --output {output.mod_p}/chr{wildcards.chr}_imputed_${{CHR}}_${{REGS}}_${{REGE}}.bcf
        done < {input.chunks}
        """

#ligate chunks of the same chromosome for ancient individuals
#ligation of the imputed chunks is performed using the GLIMPSE2_ligate tool with an ordered list of the imputed chunks       
rule glimpse_ligate_ancient:
    input:
        impute_dir = "results/03_ancient/{ancsample}/split_chromosomes/chr{chr}"
    output:
        ligate_txt =  "results/03_ancient/{ancsample}/split_chromosomes/ligate/{ancsample}list.chr{chr}.txt",
        ligate_bcf = "results/03_ancient/{ancsample}/split_chromosomes/ligate/{ancsample}_chr{chr}_ligated.bcf",
        ligate_vcf = "results/03_ancient/{ancsample}/split_chromosomes/ligate/{ancsample}_chr{chr}_ligated.vcf.gz"
    shell:
        """
        mkdir -p "results/03_ancient/{wildcards.ancsample}/split_chromosomes/ligate"

        # directly produce the list of files in the right order of the imputed chunks
        ls -1v {input.impute_dir}/chr{wildcards.chr}_imputed_*.bcf > {output.ligate_txt}
        
        GLIMPSE2_ligate --input {output.ligate_txt} \
                        --output {output.ligate_bcf}

        bcftools view {output.ligate_bcf} -Oz -o {output.ligate_vcf}
        """

#ligate chunks of the same chromosome for modern individuals
rule glimpse_ligate_modern:
    input:
        impute_dir = "results/02_modern/{mdsample}/split_chromosomes/chr{chr}"
    output:
        ligate_txt =  "results/02_modern/{mdsample}/split_chromosomes/ligate/{mdsample}list.chr{chr}.txt",
        ligate_bcf = "results/02_modern/{mdsample}/split_chromosomes/ligate/{mdsample}_chr{chr}_ligated.bcf",
        ligate_vcf = "results/02_modern/{mdsample}/split_chromosomes/ligate/{mdsample}_chr{chr}_ligated.vcf.gz"
    shell:
        """
        mkdir -p "results/02_modern/{wildcards.mdsample}/split_chromosomes/ligate"

        ls -1v {input.impute_dir}/chr{wildcards.chr}_imputed_*.bcf > {output.ligate_txt}
        
        GLIMPSE2_ligate --input {output.ligate_txt} \
                        --output {output.ligate_bcf}

        bcftools view {output.ligate_bcf} -Oz -o {output.ligate_vcf}
        """

#merge the ligated vcf files of ancient and modern individuals by chromosome
#create a merged vcf file for each chromosome containing information for all individuals as it's required by ancIBD
rule merge_vcf_chr:
    input:
        ancient_vcfs = expand(rules.glimpse_ligate_ancient.output.ligate_vcf, 
                               ancsample=anc_sample_df['sample_name'], chr=['{chr}']),
        modern_vcf = expand(rules.glimpse_ligate_modern.output.ligate_vcf, 
                             mdsample=[mdsample], chr=['{chr}'])
    output:
        merged_vcf = "results/05_h5/chr{chr}.vcf.gz"
    shell:
        """
        mkdir -p results/05_h5

        for vcf in {input.ancient_vcfs}; do
            if [ ! -f "$vcf.tbi" ] || [ ! -f "$vcf.csi" ]; then
                bcftools index -f "$vcf"
            fi
        done

        for vcf in {input.modern_vcf}; do
            if [ ! -f "$vcf.tbi" ] || [ ! -f "$vcf.csi" ]; then
                bcftools index -f "$vcf"
            fi
        done

        bcftools merge {input.ancient_vcfs} {input.modern_vcf} -o {output.merged_vcf}
        bcftools index {output.merged_vcf}
      """

#convert the merged vcf file to hdf5 format which is required by ancIBD
#the funcion ancIBD.IO.prepare_h5.vcf_to_1240K_hdf runs the transformation from VCF and outputs a hdf5 for 1240k SNPs
#path_vcf: Path of an intermediate VCF file - which is internally filtered to 1240k data
#path_h5: Path of the output HDF5 files
#marker_path: Path of the 1240k SNPs to use
#map_path: Path of the map file to use
rule vcf_to_hdf5:
    input:
        in_vcf = rules.merge_vcf_chr.output.merged_vcf,
        marker = "resources/marker/snps_bcftools_ch{chr}.csv",
        map_file = "resources/map/v51.1_1240k.snp"
    output:
        vcf = "results/05_h5/1240k_chr{chr}.vcf.gz",
        h5 = "results/05_h5/all_chr{chr}.h5"
    run:
        import sys
        import os
        import matplotlib.cm as cm
        import pandas as pd
        from ancIBD.IO.prepare_h5 import vcf_to_1240K_hdf
        
        base_path = os.getcwd()
        chr = int(wildcards.chr)  
        
        vcf_to_1240K_hdf(
            in_vcf_path = os.path.join(base_path, f"results/05_h5/chr{chr}.vcf.gz"),
            path_vcf = os.path.join(base_path, f"results/05_h5/1240k_chr{chr}.vcf.gz"),
            path_h5 = os.path.join(base_path, f"results/05_h5/all_chr{chr}.h5"),
            marker_path = os.path.join(base_path, f"resources/marker/snps_bcftools_ch{chr}.csv"),
            map_path = os.path.join(base_path, f"resources/map/v51.1_1240k.snp"),
            col_sample_af = "",
            buffer_size=20000, 
            chunk_width=8, 
            chunk_length=20000,
            ch=chr
        )

#run ancIBD on samples to output all IBD segments
#get lists of sample names (iids) from sample field from HDF5 file content which will be used later
#folder_in: The path of the hdf5 files used for IBD calling
#iids: All iids to load
#p_col: This specifies which field to use for reference allele frequencies. set ‘variants/RAF’ which are the 1000G reference allele frequencies
#other parameters are set according to the recommandation from ancIBD tool which work perfectly for aDNA data but also apply to modern data 
rule call_ibd:
    input:
        h5_files = rules.vcf_to_hdf5.output.h5
    output:
        ibd_ind = "results/06_ibd/ch{chr}.tsv"
    params:
        h5_folder = "results/05_h5",
        out_folder = "results/06_ibd"
    run:
        import sys as sys
        import matplotlib.cm as cm
        import pandas as pd
        import os as os
        import h5py
        import glob
        import numpy as np
        from ancIBD.run import hapBLOCK_chroms

        
        base_path = os.getcwd()
        chr = int(wildcards.chr) 
        os.makedirs(params.out_folder, exist_ok=True)

        h5_pattern = os.path.join(base_path, params.h5_folder, "*chr*.h5")
        h5_files = sorted(glob.glob(h5_pattern))
        first_h5 = h5_files[0]
        with h5py.File(first_h5, 'r') as f:
            iids = [s.decode('utf-8').split('\x00')[0] if isinstance(s, bytes) 
                    else s.split('\x00')[0] for s in f['samples'][:]]


        
        df_ibd = hapBLOCK_chroms(folder_in=os.path.join(base_path, f"results/05_h5/all_chr"),
                                iids=iids, run_iids=[],
                                ch=chr, folder_out=os.path.join(base_path, f"results/06_ibd"),
                                output=False, prefix_out='', logfile=False,
                                l_model='h5', e_model='haploid_gl2', h_model='FiveStateScaled', t_model='standard',
                                p_col='variants/RAF',
                                ibd_in=1, ibd_out=10, ibd_jump=400,
                                min_cm=6, cutoff_post=0.99, max_gap=0.0075)

#combine IBD calls from all chromosomes and postprocess output into single summary table                              
rule combine_ibd_results:
    input:
        ibd_files = expand("results/06_ibd/ch{chr}.tsv", chr=range(1, 23))

    output:
        ibd_all = "results/06_ibd/ch_all.tsv",
        ibd_summary = "results/06_ibd/ibd_ind.d220.tsv"
    params:
        out_folder = "results/06_ibd"
    run:
        import os
        import pandas as pd
        from ancIBD.IO.ind_ibd import combine_all_chroms
        from ancIBD.IO.ind_ibd import create_ind_ibd_df
        
        base_path = os.getcwd()
        
        combine_all_chroms(chs=range(1,23),
                            folder_base=os.path.join(base_path, f"results/06_ibd/ch"),
                            path_save=os.path.join(base_path, f"results/06_ibd/ch_all.tsv"))

        df_res = create_ind_ibd_df(ibd_data = os.path.join(base_path, f"results/06_ibd/ch_all.tsv") ,
                      min_cms = [8], snp_cm = 220, min_cm = 5, sort_col = 0,
                      savepath = os.path.join(base_path, f"results/06_ibd/ibd_ind.d220.tsv"))
        df_ibd = pd.read_csv(os.path.join(base_path, f"results/06_ibd/ibd_ind.d220.tsv"), sep="\t")

#sort and merge the results to get the final output
rule process_ibd_results:
    input:
        ibd_ind = rules.combine_ibd_results.output.ibd_summary,
        ch_all = rules.combine_ibd_results.output.ibd_all
    output:
        processed_ibd = "results/06_ibd/processed_ibd_report_{mdsample}.tsv"
    params:
        modern_id = config['modern_ibd_id']  # get the modern individual id from config file
    run:
        import pandas as pd
        
        def process_anc_ibd_data(ibd_ind_file, ch_all_file, modern_id, output_file):
 
            # read the summary statistics table and the overall file for IBD calls from each chromosome
            ibd_ind_df = pd.read_csv(ibd_ind_file, sep='\t')
            ch_all_df = pd.read_csv(ch_all_file, sep='\t')
            
            # select rows include modern individual for the summary statistics table
            modern_rows = ibd_ind_df[(ibd_ind_df['iid1'] == modern_id) | (ibd_ind_df['iid2'] == modern_id)].copy()
            
            # make sure the modern individual is always in the first column, if not swap the columns
            modern_rows['std_iid1'] = modern_rows['iid1']
            modern_rows['std_iid2'] = modern_rows['iid2']
            swap_mask = modern_rows['iid1'] != modern_id
            modern_rows.loc[swap_mask, ['std_iid1', 'std_iid2']] = modern_rows.loc[swap_mask, ['iid2', 'iid1']].values
            
            # sort the table by value of sum_IBD(>8cM), from high to low
            modern_rows = modern_rows.sort_values(by='sum_IBD>8', ascending=False)
            
            # do the same for the overall file for IBD calls from each chromosome to get the standard format so that we can merge them without problems
            ch_modern_df = ch_all_df[
                (ch_all_df['iid1'] == modern_id) | (ch_all_df['iid2'] == modern_id)
            ].copy()
            
            ch_modern_df['std_iid1'] = ch_modern_df['iid1']
            ch_modern_df['std_iid2'] = ch_modern_df['iid2']
            swap_mask = ch_modern_df['iid1'] != modern_id
            ch_modern_df.loc[swap_mask, ['std_iid1', 'std_iid2']] = ch_modern_df.loc[swap_mask, ['iid2', 'iid1']].values
            
            # merge the two dataframes on std_iid1 and std_iid2
            merged_df = pd.merge(
                modern_rows,
                ch_modern_df,
                on=['std_iid1', 'std_iid2'],
                how='left',
                suffixes=('', '_ch')
            )
            
            # get the simplified version of the individual ids
            def extract_id_from_path(id_str):
                if isinstance(id_str, str) and '/' in id_str:
                    return id_str.split('/')[-1]
                return id_str
            
            merged_df['std_iid2'] = merged_df['std_iid2'].apply(extract_id_from_path)
            
            # select which columns to keep for the final output
            results_df = merged_df[['std_iid1', 'std_iid2', 'ch', 'StartBP', 'EndBP', 'sum_IBD>8']].copy()
            
            results_df = results_df.rename(columns={
                'std_iid1': 'iid1',
                'std_iid2': 'iid2',
                'ch': 'chr',
                'StartBP': 'start',
                'EndBP': 'end'
            })
            
            results_df.to_csv(output_file, sep='\t', index=False)            
            return results_df
        
        # call the function process_anc_ibd_data to process the data and get the result
        process_anc_ibd_data(input.ibd_ind, input.ch_all, params.modern_id, output.processed_ibd)